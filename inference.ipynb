{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample the posterior distribution, we alternate between sampling the discrete variable $z_t$ and sampling the conditional for the parameters $A_k,Q_k$.\n",
    "\n",
    "Let's say we are given $\\Pi$ and $z_t$. Then, we have a conditional posterior distribution on the $A_k$ and $Q_k$, because we can group the data based on the state at time $t$, and have $K$ separate Bayesian multivariate linear regression.\n",
    "\n",
    "# Bayesian MultiVariate Linear regression\n",
    "\n",
    "Start from the general case: we have\n",
    "\n",
    "$$\n",
    "Y = AX + E\n",
    "$$\n",
    "\n",
    "where $Y$ is a $m \\times n$ matrix of $n$ observation of $m$ dependent variables, $X$ is $k \\times n$ ($k-1$ explanatory variables, we add the dummy variable), and $E$ is $m \\times n$ where each column is drawn from a multivariate gaussian: $E_n \\sim \\mathcal{N}(0,Q)$. The likelihood is gaussian in $Y-AX$:\n",
    "\n",
    "$$\n",
    "P(Y|X,A,Q) \\sim \\mathcal{N}(Y-AX,Q)\n",
    "$$\n",
    "\n",
    "Now we want a conjugate prior for both $A$ and $Q$, so that the posterior is functionally identical. It can be done by imposing an inverse Wishart prior on the covariance matrix $Q$ and a matrix normal distribution on $A$, which is some sort of normal distribution but on the vectorized matrix (a \"flattened\" vector version of the matrix $Q$):\n",
    "\n",
    "$$\n",
    "P(A,Q) = P(A|Q)P(Q) = \\mathcal{W}^{-1}(\\Psi,\\nu) \\times \\mathcal{MN}(M,\\Lambda,Q) \\\\\n",
    "\\mathcal{W}^{-1}(\\Psi,\\nu) = |\\Psi|^{\\nu/2}|Q|^{(-m+\\nu+1)/2}\\exp{(-\\frac{1}{2}Tr [ \\Psi Q^{-1} ] )} \\\\\n",
    "\\mathcal{MN}(M,\\Lambda,Q) = |Q|^{-m/2}|\\Lambda|^{-k/2} \\exp{-\\frac{1}{2}Tr[(A-M)^T Q^{-1}(A-M)\\Lambda^{-1}]}\n",
    "$$\n",
    "\n",
    "It can be shown that for this likelihood, the posterior distribution is again a matrix normal times a wishart with updated parameters:\n",
    "\n",
    "$$\n",
    "M' = \\hat{A} XX^T \\Lambda' + M \\Lambda^{-1} \\Lambda' \\\\\n",
    "\\Lambda' = (\\Lambda^{-1}+ XX^T)^{-1}\\\\\n",
    "\\Psi' = \\Psi + M\\Lambda^{-1}M^T + YY^T-A'(???)\\\\\n",
    "\\nu'= \\nu + T\n",
    "$$\n",
    "\n",
    "where $\\hat{A}$ is the OLS solution:\n",
    "\n",
    "$$\n",
    "\\hat{A} = YX^T(XX^T)^{-1}\n",
    "$$\n",
    "\n",
    "## SLDS\n",
    "\n",
    "In the case of SLDS, we have $K$ separate linear regressions, and if we have $k$ dynamical variables, then $Y=X_{t}$ is $m \\times n$, $X=X_{t-1}$ is $(m+1) \\times n$ to account for the bias, and the coefficients matrix is then $m \\times (m+1)$\n",
    "\n",
    "The SLDS is as such:\n",
    "\n",
    "$$\n",
    "X_t = A_{z_t} X_{t-1} + \\epsilon_{z_t}\n",
    "$$\n",
    "\n",
    "So, if we define all the steps where the $k$-th dynamic was used:\n",
    "\n",
    "$$\n",
    "t_k = \\{ t > 1 | z_t = k \\}, \\;\\;\\; |t_k| = T_k\n",
    "$$\n",
    "\n",
    "we can group data as \n",
    "\n",
    "$$\n",
    "X^{(k)} = \\{ x_{t-1} | t \\in t_k \\} \\\\\n",
    "Y^{(k)} = \\{ x_t | t \\in t_k \\}\n",
    "$$\n",
    "\n",
    "and use those to update the parameters $\\{M'_k,\\Lambda_k',\\Psi_k',\\nu_k' \\}$ for each of the posterior distributions for $A_k,Q_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import matrix_normal\n",
    "# the matrix_normal.rvs() accepts:\n",
    "# mean : what we called M\n",
    "# rowcov : the (m x m) row covariance matrix, what we called Q\n",
    "# colcov : the (m+1) x (m+1) columns covariance matrix, what we called lambda\n",
    "\n",
    "from scipy.stats import invwishart\n",
    "# the invwishart.rvs() accepts\n",
    "# df: what we called nu\n",
    "# scale: what we called psi, same shape as Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden state\n",
    "\n",
    "What about the conditional on $z_t$ and $\\Pi$? Given $z_{t-1}$, the next state is the outcome of the categorical distribution $\\Pi_{z_{t-1}}$ (the $z_{t-1}$ row of the transition matrix). \n",
    "\n",
    "### $\\Pi$\n",
    "By assigning a Dirichlet prior on each row of the transition matrix:\n",
    "$$\n",
    "\\Pi_{k} \\sim Dir(\\alpha_k)\n",
    "$$ \n",
    "\n",
    "given $z_t$ (so given a sequence of observed transitions) the posterior distributions are again Dirichlet like (since it's the conjugate prior for a categorical distribution) with updated parameters:\n",
    "\n",
    "$$\n",
    "\\Pi_{k} | all \\sim Dir(\\alpha_k + n_k)\n",
    "$$\n",
    "\n",
    "where $n_k$ is a vector containing the number of transition observed from state $k$ to any other state.\n",
    "\n",
    "### $z_t$\n",
    "\n",
    "Given $z_{t-1}$, the process is similar to a mixture distributions model with latent variable; when calculating the probability of using a certain $z_t$, we have:\n",
    "\n",
    "$$\n",
    "P(z_t,\\Pi_{z_{t-1}},all) = P(z_t,\\Pi_{z_{t-1}}|all)P(all)\n",
    "$$\n",
    "\n",
    "now, let $z_{t_k}$ be the probability that $z_t = k$, and $z_{t_{-k}}$ the vector of probabilities for all other realizations. We can write\n",
    "\n",
    "$$\n",
    "P(z_t,\\Pi_{z_{t-1}}|all)P(all) = P(z_{t_k}|z_{t_{-k}},\\Pi_{z_t},all)P(z_{t_{-k}},\\Pi_{z_t}|all)P(all)\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "P(z_{t_k}|z_{t_{-k}},\\Pi_{z_t},all) = \\frac{P(z_t,\\Pi_{z_{t-1}}|all)}{P(z_{t_{-k}},\\Pi_{z_t}|all)}\n",
    "$$\n",
    "\n",
    "and that leads to\n",
    "\n",
    "$$\n",
    "P(z_t = k) = \\frac{r_{tk}}{\\sum_k r_{tk}}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "r_{tk} = \\Pi_{z_{t-1},k} |Q_k|^{-1/2} \\exp \\{ -\\frac{1}{2} (x_t - A_k x_{t-1})^T Q_k^{-1} (x_t - A_k x_{t-1}) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet\n",
    "# dirichlet.rvs() accepts\n",
    "# alpha: vector of the same dimensionality of the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "To generate samples, we start by initializing the sequence $z_t$ randomly, and $\\forall k$ sample $\\Pi_k, A_k, Q_k$ from the priors. Then loop:\n",
    "\n",
    "1. $\\forall k$, compute $\\{ n_k, X^{(k)}, Y^{(k)}, M'_k, \\Lambda'_k,\\Psi'_k,\\nu'_k  \\}$\n",
    "2. $\\forall k$, sample $\\Pi_k \\sim Dir(\\alpha_k + n_k)$\n",
    "3. $\\forall k$, sample $A_k,Q_k \\sim MNIW(M'_k, \\Lambda'_k,\\Psi'_k,\\nu'_k)$\n",
    "4. $\\forall t=2,\\dots,T$ sample $z_t$ from $P(z_t = k) = r_{tk}/\\sum_k r_{tk}$\n",
    "\n",
    "The sampler alternates sampling the latent variable and sampling the posterior distribution of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "X_t = np.load('./simulation/x.npy')\n",
    "Z_true = np.load('./simulation/z.npy')\n",
    "\n",
    "m = X_t.shape[0]\n",
    "T = X_t.shape[1]\n",
    "\n",
    "# assert the number of states; let's choose them to be\n",
    "# the true number of latent states that we know\n",
    "\n",
    "K = len(np.unique(Z_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples\n",
    "N_samples = 2000\n",
    "\n",
    "# initialize samples arrays\n",
    "A = np.empty( (N_samples,K,m,m+1) )\n",
    "Q = np.empty( (N_samples,K,m,m)   )\n",
    "\n",
    "Z = np.empty( (N_samples,T) )\n",
    "Pi = np.empty( (N_samples,K,K) )\n",
    "\n",
    "# prior parameters\n",
    "\n",
    "M = np.zeros( (K,m,m+1) )\n",
    "\n",
    "# identity matrices\n",
    "Lambda = np.tile(np.eye(m+1),(K,1,1))\n",
    "Psi = np.tile(np.eye(m),(K,1,1))\n",
    "\n",
    "nu = np.ones( (K) ) * (m+1)\n",
    "# uniform distribution for each k\n",
    "alpha = np.ones((K,K))\n",
    "\n",
    "\n",
    "# posterior parameters\n",
    "M_new = np.empty_like(M)\n",
    "Lambda_new = np.empty_like(Lambda)\n",
    "Psi_new = np.empty_like(Psi)\n",
    "nu_new = np.empty_like(nu)\n",
    "alpha_new = np.empty_like(alpha)\n",
    "\n",
    "n = np.empty((K,K),dtype=np.int8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize first sample\n",
    "\n",
    "Z[0] = np.random.randint(0,K,T)\n",
    "\n",
    "for k in range(K):\n",
    "\n",
    "    Q[0][k] = invwishart.rvs(df=nu[k],scale=Psi[k])\n",
    "    A[0][k] = matrix_normal.rvs(mean=M[k],rowcov=Q[0][k], colcov=Lambda[k])\n",
    "    Pi[0][k] = dirichlet.rvs(alpha=alpha[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "\n",
    "for i in range(1,2):\n",
    "\n",
    "    for k in range(K):\n",
    "        \n",
    "        # find the indices of z_t = k\n",
    "        t_k = np.argwhere(Z[i-1] == k).reshape(-1)\n",
    "        # indices immediately after, removing out of boundaries\n",
    "        t_k_next = t_k + 1\n",
    "        t_k_next = t_k_next[t_k_next < T]\n",
    "        \n",
    "        # get X, but exclude first point\n",
    "        # also add dummy variable\n",
    "        X = X_t[:,t_k[t_k>0]-1]\n",
    "        X = np.concatenate((X,np.ones(X.shape[1]).reshape(1,-1)),axis=0)\n",
    "        \n",
    "        # get Y\n",
    "        Y = X_t[:,t_k[t_k>0]]\n",
    "        \n",
    "        # calculate n_k\n",
    "        for l in range(K):\n",
    "            n[k,l] = (Z[i-1][t_k_next] == l).sum()\n",
    "        \n",
    "        # compute new posterior parameters\n",
    "\n",
    "        # M_new[k] = M[k]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
