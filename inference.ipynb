{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian linear regression\n",
    "\n",
    "Let's start with the Bayesian approach for a linear system. We have\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_t = A \\mathbf{x}_t + \\mathbf{\\epsilon}_t\n",
    "$$\n",
    "\n",
    "where, in general, $\\mathbf{y}_t \\in \\mathbb{R}^M$, $\\mathbf{x}_t \\in \\mathbb{R}^N$, and $\\mathbf{\\epsilon}_t \\sim \\mathcal{N}(0,Q)$. But we group all the observations in matrices, so that $Y,X,E$ have as their $t$-th column their observations at time $t$.\n",
    "\n",
    "$$\n",
    "Y_{it} = \\sum_j A_{ij} X_{jt} + E_{it}\n",
    "$$\n",
    "\n",
    "or simply \n",
    "\n",
    "$$\n",
    "Y = AX + E\n",
    "$$\n",
    "\n",
    "Let's assume that $y_t$ and $x_t$ have zero mean (we can always shift them so that it's true), so that every row of these matirces have zero mean.\n",
    "\n",
    "The likelihood of observing $X,Y$ is then a multivariate gaussian \"centered\" at the linear combination:\n",
    "\n",
    "$$\n",
    "f(X,Y|A,Q) = |Q|^{-T/2} \\exp{(-\\frac{1}{2}Tr[(Y-AX)^T Q^{-1} (Y-AX)])}\n",
    "$$\n",
    "\n",
    "The max-likelihood solution is the OLS one:\n",
    "\n",
    "$$\n",
    "\\hat{A} = YX^T (XX^T)^{-1}\n",
    "$$\n",
    "\n",
    "### Prior\n",
    "\n",
    "If we introduce a conjugate Matrix-normal inverse-Wishart prior:\n",
    "\n",
    "$$\n",
    "A,Q \\sim MNIW(M,\\Lambda,\\Psi,\\nu)\n",
    "$$\n",
    "\n",
    "_(MNIW formulazza)_\n",
    "\n",
    "Then the posterior for $A,Q$ is again a MNIW with\n",
    "\n",
    "$$\n",
    "M' = \\hat{A}XX^T \\Lambda' +M\\Lambda^{-1} \\Lambda' \\\\\n",
    "\\Lambda' = (\\Lambda^{-1}+XX^T)^{-1} \\\\\n",
    "\\Psi' = \\Psi + M\\Lambda^{-1}M^T + YY^T - A' \\Lambda'^{-1} A'^T \\\\\n",
    "\\nu' = \\nu + T\n",
    "$$\n",
    "\n",
    "### VAR (vector autoregressive process)\n",
    "\n",
    "Vector autoregression is a dynamical model in which the $t+1$-th vector of dynamical variables is dependent on their previous values (more generally, $p$ steps back, but here we consider $p=1$):\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_t = A\\mathbf{x}_{t-1} + \\mathbf{\\epsilon}_t\n",
    "$$\n",
    "\n",
    "where again the $\\epsilon$ is drawn from a multivariate gaussian. All the previous considerations about bayesian linear models apply here.\n",
    "\n",
    "## Switching linear systems\n",
    "\n",
    "Now, a latent categorical variable $z_t$, which follows a simple Markovian process, determines the dynamical matrix $A_{z_t}$.\n",
    "\n",
    "The likelihood for the data observed is now conditioned on the value of $z_t$, i.e. it's a _mixture_ of models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 3\n",
      "M = 2\n",
      "N = 1000\n"
     ]
    }
   ],
   "source": [
    "x = np.load('./simulation/x.npy')\n",
    "z = np.load('./simulation/z.npy')\n",
    "\n",
    "print('K =',np.unique(z).size)\n",
    "print('M =',x.shape[0])\n",
    "print('N =',x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
