{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_spd_matrix  #to generate covariance matrices\n",
    "from numpy.linalg import det\n",
    "from scipy.stats import special_ortho_group\n",
    "import collections\n",
    "from numpy.random import dirichlet\n",
    "from scipy.stats import invwishart, matrix_normal\n",
    "from numpy.linalg import inv,det\n",
    "from numpy import sqrt\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uploading data\n",
    "meno_data = 10\n",
    "X_data = np.load('./simulation/x.npy')[:,:meno_data]\n",
    "z_data = np.load('./simulation/z.npy')[:meno_data]\n",
    "\n",
    "K = np.unique(z_data).size\n",
    "M = X_data.shape[0]\n",
    "T = X_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFERENCE AND GIBBS SAMPLER\n",
    "\n",
    "$\\pi_k$ is the $k^{th}$ row of the transition matrix and it is a vector containing the probabilities to reach another discrete state starting from $z = k$.\\\n",
    "Thus, it is a vector of the type: $\\pi_k = (p_{k,0}, p_{k,1},\\dots,p_{k,K-2},p_{k,K-1})$. \n",
    "\n",
    "The problem we are facing is conceptually similar to the *mixture of gaussians model* apart from the fact that there are $K$ vectors like $\\pi_k$, while in the *mixture of gaussians model* there is only one.\n",
    "So, we can write $P(z_t = q | z_{t-1} = k) = p_{k,q}$ or $P(z_t | z_{t-1} = k) = cat(\\pi_k)$ for all $k$s. \n",
    "\n",
    "We choose as prior for every $\\pi_k$ a Dirichlet distribution i.e. the conjugate prior of the categorical:\n",
    "$\\boldsymbol{\\pi}_k | \\alpha_k \\sim \\operatorname{Dir}\\left(\\boldsymbol{\\alpha}_k\\right)$ with $\\boldsymbol{\\alpha_{k}} = \\boldsymbol{1} \\in \\mathbb{R}^{K}$ for an uniformative prior.\\\n",
    "In this way the posterior is again a Dirichlet: \n",
    "$P(\\boldsymbol{\\pi}_k | z_t, z_{t-1} = k) = \\operatorname{Dir}\\left(\\boldsymbol{\\alpha}_k + \\boldsymbol{n}_k \\right)$\n",
    "where $\\boldsymbol{n}_k = (n_{k,0}, \\dots, n_{k,K-2}, n_{k,K-1})$ is the vector containing the number of times is observed a transition $k \\rightarrow q$ for every $q$.\n",
    "\n",
    "It looks like the whole procedure is an hybrid between *soft clustering* and *multivariate bayesian regression*: given all the experimental data $(\\boldsymbol{X},\\boldsymbol{Y})$ we want to divide them in $K$ clusters obtaining\n",
    "$(\\boldsymbol{X^{(k)}},\\boldsymbol{Y^{(k)}})\\; \\forall k$. Each cluster is defined by two matrices $A_k$ and $Q_k$ and we want to perform a linear regression on them to find the matrices. Eventually we also want the probability that the next point will be sampled from a given cluster i.e. the transition matrix and the distribution of the variable $z_t$.\n",
    "\n",
    "Let's go go on with the $K$ linear regression (one for each cluster). It is worth to notice is that the linear regreassions are separated: $X^{(k)}$ does not interact with $X^{(k-1)}$. What I mean is that at this point the math of the problem says that we are going to perform linear regressions on $k$ systems of the form: $X_{s}^{(k)} = A_K X_{s - 1}^{(k)} + b_k$, where the time $s$ is redifined on the subset $X^{(k)}$ of $X$.\n",
    "We want to perform the regressions in homogeneous coordinates, so the dynamics become $Y_{s}^{(k)} = A_K X_{s - 1}^{(k)}$, but now $Y_{s}^{(k)} \\in \\mathbb{R}^{M}$, $X_{s - 1}^{(k)} \\in \\mathbb{R}^{M + 1}$ and $A_{k} \\in \\mathbb{R}^{M x (M + 1)}$ and its last column is the vector $b_k$.\n",
    "\n",
    "Let's write something more on the bayesian linear regression. I am dropping the index $k$ but it must be understood that everything is done for the $k^{th}$ bayesian linear regression.\n",
    "We choose as priors: $P(A|Q) = \\mathcal{M} \\mathcal{N}_{M,M+1}(M_0,Q,\\Lambda_{0}^{-1})$ where $M_0 \\in \\mathbb{R}^{M,M + 1}$ and $\\Lambda_{0}^{-1} \\in \\mathbb{R}^{M + 1,M + 1}$ and also $P(Q) = \\mathcal{W}^{-1}\\left(\\mathbf{V}_0, \\boldsymbol{\\nu}_0\\right)$\n",
    "where $\\mathbf{V}_0$ has the same dimensions as $Q$ ($Q \\in \\mathbb{R}^{MxM}$) and it is positive definite, $\\boldsymbol{\\nu}_0$ is a number.\n",
    "Using the priors above, the posteriors are:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{Q} |\\mathbf{Y}, \\mathbf{X}) \\sim \\mathcal{W}^{-1}\\left(\\mathbf{V}_T, \\nu_T\\right)\\\\\n",
    "P(\\mathbf{A} | \\mathbf{Y}, \\mathbf{X}, \\mathbf{Q}) \\sim \\mathcal{M} \\mathcal{N}_{M, M+1}\\left(\\mathbf{M}_T,\\mathbf{Q}, \\mathbf{\\Lambda}_T^{-1}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\\begin{aligned}\n",
    "& \\mathbf{V}_T=\\mathbf{V}_0+\\left(\\mathbf{Y}-\\mathbf{M_T X}\\right)\\left(\\mathbf{Y}-\\mathbf{M_T X}\\right)^{\\top} + \\left(\\mathbf{M}_T -\\mathbf{M}_0\\right) \\mathbf{\\Lambda}_0\\left(\\mathbf{M}_T -\\mathbf{M}_0\\right)^{\\top} \\\\\n",
    "& \\boldsymbol{\\nu}_T =\\boldsymbol{\\nu}_0 + T \\\\\n",
    "& \\mathbf{M}_T^{\\top} = (\\mathbf{X} \\mathbf{X}^{\\top}+\\mathbf{\\Lambda}_0)^{-1} (\\mathbf{X} \\mathbf{Y}^{\\top} +\\mathbf{\\Lambda}_0 \\mathbf{M}_0^{\\top})\\\\\n",
    "& \\mathbf{\\Lambda}_T=\\mathbf{X} \\mathbf{X}^{\\top} + \\mathbf{\\Lambda}_0\n",
    "\\end{aligned}\n",
    "\n",
    "(In this way it should be ok, it is different from wikipedia 'bayesian multivariate linear regression' because they start with a different equation for the linear regression).\n",
    "Where $T$ is actually $T_k$ and it is the number of columns f the matrix $Y^{(k)}$.\\\n",
    "What make sense to speed up the convergence of the Gibbs sampler is to compute $\\Lambda_{T}$ using itself at the previous interation instead of $\\Lambda_{0}$ and the same for the other parameters.\\\n",
    "Furthermore is convenient to firstly compute $\\nu_T$ and $\\Lambda_T$ and then plague them in the other to parameters.\n",
    "\n",
    "Eventually we have to sample the the whole vector $z_t$ from its posterior:\n",
    "\n",
    "$$\n",
    "\\operatorname{P}\\left(z_t=k | z_{t-1},ALL\\right)=\\frac{r_{t k}}{\\sum_k r_{t k}} \\; \\text{where} \\;\n",
    "r_{t k}=p_{z_{t-1},k}\\left|Q_k\\right|^{-1 / 2} \\exp \\left\\{-\\frac{1}{2}\\left(\\mathbf{y}_t -A_k \\mathbf{x}_{t}\\right)^T Q_k^{-1}\\left(\\mathbf{y}_t - A_k \\mathbf{x}_{t} \\right)^T \\right\\}\\\\\n",
    "\\text{or equivalentely} \\; z_t \\sim \\operatorname{cat}\\left(p_t^{\\prime}\\right) \\; \\text{where} \\; p_t^{\\prime} = \\operatorname{P}\\left(z_t | z_{t-1},ALL\\right) \\; \\forall t\n",
    "$$\n",
    "\n",
    "In some sense it is a probability that takes into account the distance between the cluster labeled $k$ and the point $x_t$.\n",
    "The whole procedure must be carried on till convergence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing data in homogeneous coordinates\n",
    "X = X_data[:, 0:T-1] #this X_{t-1} so times run between [0,T-1]\n",
    "\n",
    "#attaching ones\n",
    "X = np.vstack((X,np.ones(T-1))) #dimensions (M+1) x (T-1)\n",
    "\n",
    "Y = X_data[:, 1:T] #this is X_t so times run between [1,T], dimensions M x (T-1)\n",
    "\n",
    "n_iterations = 15 #number of iterations in the Gibbs sampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start mfs.The code below must be reorganised and the loops in k grouped\n",
    "\n",
    "z = np.zeros((n_iterations, T - 1), dtype = np.int8)\n",
    "\n",
    "#uniform random vector z_t of length T\n",
    "z[0,:] = np.random.choice(K, size = T - 1, p = None) #for each couple (X_t,Y_t) there must be a value of z_t\n",
    "\n",
    "#N_k is a matrix KxK containing as rows the vectors n_k\n",
    "N_k = np.zeros((K,K))\n",
    "\n",
    "#filling the matrix N_k\n",
    "for i in range(T - 2):\n",
    "    N_k[z[0,i]][z[0,i+1]] += 1 \n",
    "\n",
    "#now I label the experimental point based on the extracted z_t, will do it as a vocabolary of matrices.\n",
    "X_k = {}\n",
    "Y_k = {} #it is X_k translated of one time-step (without homogeneous coordinates)\n",
    "for k in np.arange(K):\n",
    "    \n",
    "    k_indices = np.where(z[0,:] == k)\n",
    "    X_k[str(k)] = X[:,k_indices[0]] #k_indices[0] otherwise python adds a void extra_dimension\n",
    "    Y_k[str(k)] = Y[:,k_indices[0]]\n",
    "\n",
    "#now I need to initialise nu_k, L_k, V_k, A_k, M_k\n",
    "#we need dictionaries of dictionaries: first index being k and the second the number of the iteration s.\n",
    "Q_k = collections.defaultdict(dict)\n",
    "L_k = collections.defaultdict(dict)\n",
    "V_k = collections.defaultdict(dict)\n",
    "nu_k = collections.defaultdict(dict)\n",
    "A_k = collections.defaultdict(dict)\n",
    "M_k = collections.defaultdict(dict)\n",
    "\n",
    "\n",
    "for k in np.arange(K): #I do not really understand the meaning of this initialization\n",
    "    nu_k[str(k)]['0'] = K #greater or equal then the dimension of the scale matrix\n",
    "    L_k[str(k)]['0'] = make_spd_matrix(M+1, random_state = seed) #np.ones((M+1,M+1)) questo non funziona perchè è singolare\n",
    "    V_k[str(k)]['0'] = make_spd_matrix(M, random_state = seed) #this must be positive semidefinite\n",
    "    M_k[str(k)]['0'] = np.zeros((M,M+1))\n",
    "\n",
    "#now as in 'details' randomly sample the transition matrix, Q_k, A_k from the priors \n",
    "Q_k = collections.defaultdict(dict)\n",
    "\n",
    "PI_k = collections.defaultdict(dict)\n",
    "\n",
    "#parameter for the dirichlet\n",
    "alp = np.ones(K) \n",
    "\n",
    "for k in np.arange(K): #Each dictionary of PI_k correspond to the same k and so to the same row of the transition matrix.\n",
    "    PI_k[str(k)]['0'] = dirichlet(alpha = alp, size = 1).flatten()\n",
    "    \n",
    "    Q_k[str(k)]['0'] = invwishart.rvs(df = nu_k[str(k)]['0'], scale =  V_k[str(k)]['0'])\n",
    "    \n",
    "    A_k[str(k)]['0'] = matrix_normal.rvs(mean = M_k[str(k)]['0'], colcov = inv(L_k[str(k)]['0']), rowcov = Q_k[str(k)]['0'] )\n",
    "    \n",
    "#Now we have a first approximation of the transition matrix, of the matrices A_k, Q_k and the other parameters.\n",
    "#Let's start the iterations\n",
    "\n",
    "for it in range(1, n_iterations):\n",
    "    for k in np.arange(K):\n",
    "         PI_k[str(k)][str(it)] = dirichlet(alpha = alp + N_k[k,:], size = 1).flatten()\n",
    "            \n",
    "         #now I have to update L_k, nu_k, A_k, V_k, Q_k\n",
    "         L_k[str(k)][str(it)] = X_k[str(k)] @ X_k[str(k)].T +  L_k[str(k)][str(it - 1)]\n",
    "         nu_k[str(k)][str(it)] = nu_k[str(k)][str(it-1)] + Y_k[str(k)].shape[1]\n",
    "        \n",
    "         #I am using L_k[k][it] to update M_k, I think this will speed up the convergence but it is only a guess\n",
    "         M_k[str(k)][str(it)] = inv(L_k[str(k)][str(it)]) @\\\n",
    "        (X_k[str(k)] @ Y_k[str(k)].T + L_k[str(k)][str(it)] @ M_k[str(k)][str(it-1)].T)\n",
    "        \n",
    "         M_k[str(k)][str(it)] = M_k[str(k)][str(it)].T\n",
    "\n",
    "         V_k[str(k)][str(it)] = V_k[str(k)][str(it - 1)] + (Y_k[str(k)] - M_k[str(k)][str(it)] @ X_k[str(k)]) @\\\n",
    "         (Y_k[str(k)] - M_k[str(k)][str(it)] @ X_k[str(k)]).T + (M_k[str(k)][str(it)] - M_k[str(k)][str(it-1)])@\\\n",
    "         L_k[str(k)][str(it)] @ (M_k[str(k)][str(it)] - M_k[str(k)][str(it-1)]).T\n",
    "        \n",
    "         Q_k[str(k)][str(it)] = invwishart.rvs(df = nu_k[str(k)][str(it)], scale =  V_k[str(k)][str(it)])\n",
    "         A_k[str(k)][str(it)] = matrix_normal.rvs(mean = M_k[str(k)][str(it)], colcov = inv(L_k[str(k)][str(it)]), \\\n",
    "                                              rowcov = Q_k[str(k)][str(it)] )\n",
    "    #now sampling z from the posterior\n",
    "    for t in np.arange(1,T-2):\n",
    "        \n",
    "        #Pi_k[str(k)][...][...] is the probability to go from the state z_{t-1} to the state z_t = k\n",
    "        \n",
    "        p_prime = np.array([ PI_k[str(k)][str(it)][z[it,t-1]]/sqrt(det(Q_k[str(k)][str(it)])) *\\\n",
    "                            np.exp((-0.5 * (Y[:,t] - A_k[str(k)][str(it)] @ X[:,t]).T \\\n",
    "                                    @ inv(Q_k[str(k)][str(it)]) @\\\n",
    "                                    (Y[:,t] - A_k[str(k)][str(it)] @ X[:,t]))) for k in np.arange(K)])\n",
    "        \n",
    "        p_prime = p_prime/np.sum(p_prime)\n",
    "        \n",
    "        z[it,t] = np.random.choice(K, size = 1, p = p_prime)\n",
    "    \n",
    "    #recomputing stuff\n",
    "    #not finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 2 2 0 2 2 1 2]\n",
      " [0 1 2 2 0 2 2 1 0]\n",
      " [0 1 2 0 0 2 2 1 0]\n",
      " [0 0 2 0 0 2 2 1 0]\n",
      " [0 1 2 2 1 2 1 1 0]\n",
      " [0 1 2 1 0 1 1 1 0]\n",
      " [0 1 2 0 0 2 1 1 0]\n",
      " [0 1 2 2 0 2 1 2 0]\n",
      " [0 0 2 2 0 2 2 1 0]\n",
      " [0 2 0 2 0 2 1 1 0]\n",
      " [0 2 2 0 0 2 1 1 0]\n",
      " [0 0 2 2 0 2 1 1 0]\n",
      " [0 0 2 0 0 2 1 1 0]\n",
      " [0 2 2 1 2 1 2 1 0]\n",
      " [0 2 2 2 0 2 1 1 0]]\n",
      "[1.63332006e-26 6.79074172e-01 3.20925828e-01]\n"
     ]
    }
   ],
   "source": [
    "print(z[:,:])\n",
    "print(p_prime)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2b48686ecf5c051869e44bca573c1817bb1844fb32a5df209df0a7813f2e01a7"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
